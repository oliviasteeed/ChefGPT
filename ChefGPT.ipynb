{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRrs887VsmzjrLxx1+djGE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oliviasteeed/ChefGPT/blob/main/ChefGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IAT 360 Final Project\n",
        "Olivia Steed\n",
        "Welle Dias Ouambo"
      ],
      "metadata": {
        "id": "N9g_LCnHH49v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment description\n",
        "Train and/or fine-tune a CV or NLP model. This will be graded same as the CV project. Check its rubric for reference. For NLP project, this might mean a classification-like model, a generative model, summarization, translation. RAG-based chatbots, although not involving training, can belong to this category if you have proper evaluation metrics for your model. Check here for some ideas: https://huggingface.co/learn/nlp-course/en/chapter1/3?fw=ptLinks to an external site.\n",
        "\n",
        "Some differences in rubric for Option 1:\n",
        "\n",
        "Changing epoch will not be counted as a sufficient hyperparameter. Epoch needs to be stopped for the balance between overfitting and underfitting the model.\n",
        "You need to create or use dataset for your specific use-case to test it. Create a balanced dataset for testing."
      ],
      "metadata": {
        "id": "XAiQVoWtIvfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MNsGxnIH2X2"
      },
      "outputs": [],
      "source": [
        "## what needs to be done\n",
        "\n",
        "# - how to fine tune GPT-2?\n",
        "# - test fine tune on datasets Welle found - I need to look over these to familiarize myself with the format\n",
        "  # - change hyperparameters (not epoch) to get best result, document this process\n",
        "\n",
        "# - make new balanced dataset in the same format to test\n",
        "\n",
        "# - deployment they can access - how to do this?\n",
        "\n",
        "# - video showing it working"
      ]
    }
  ]
}